{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6840718,"sourceType":"datasetVersion","datasetId":3932486},{"sourceId":7357098,"sourceType":"datasetVersion","datasetId":4273118},{"sourceId":7358154,"sourceType":"datasetVersion","datasetId":4273860},{"sourceId":7362659,"sourceType":"datasetVersion","datasetId":4276957},{"sourceId":7363210,"sourceType":"datasetVersion","datasetId":4277346},{"sourceId":7363825,"sourceType":"datasetVersion","datasetId":4277776},{"sourceId":7364626,"sourceType":"datasetVersion","datasetId":4278330},{"sourceId":7385742,"sourceType":"datasetVersion","datasetId":4292898},{"sourceId":7385910,"sourceType":"datasetVersion","datasetId":4293005},{"sourceId":7387778,"sourceType":"datasetVersion","datasetId":4294259},{"sourceId":7400613,"sourceType":"datasetVersion","datasetId":4303225}],"dockerImageVersionId":30581,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:45.217945Z","iopub.execute_input":"2024-02-27T20:16:45.218998Z","iopub.status.idle":"2024-02-27T20:16:48.644773Z","shell.execute_reply.started":"2024-02-27T20:16:45.218954Z","shell.execute_reply":"2024-02-27T20:16:48.643556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:48.646942Z","iopub.execute_input":"2024-02-27T20:16:48.647274Z","iopub.status.idle":"2024-02-27T20:16:51.947900Z","shell.execute_reply.started":"2024-02-27T20:16:48.647239Z","shell.execute_reply":"2024-02-27T20:16:51.946888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install git+https://github.com/csebuetnlp/normalizer","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:51.949392Z","iopub.execute_input":"2024-02-27T20:16:51.949726Z","iopub.status.idle":"2024-02-27T20:16:56.176347Z","shell.execute_reply.started":"2024-02-27T20:16:51.949692Z","shell.execute_reply":"2024-02-27T20:16:56.175255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:56.179204Z","iopub.execute_input":"2024-02-27T20:16:56.179495Z","iopub.status.idle":"2024-02-27T20:16:56.184736Z","shell.execute_reply.started":"2024-02-27T20:16:56.179466Z","shell.execute_reply":"2024-02-27T20:16:56.184067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForPreTraining, AutoTokenizer\nfrom normalizer import normalize #buet_bert","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:56.185611Z","iopub.execute_input":"2024-02-27T20:16:56.185832Z","iopub.status.idle":"2024-02-27T20:16:56.198230Z","shell.execute_reply.started":"2024-02-27T20:16:56.185807Z","shell.execute_reply":"2024-02-27T20:16:56.197567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt \n\n# Load your dataset\ntrain_data = pd.read_excel(\"/kaggle/input/shuffled-and-new-threshold-dataset/train_shuffle_data.xlsx\")\nval_data = pd.read_excel(\"/kaggle/input/shuffled-and-new-threshold-dataset/val_shuffle_Data.xlsx\")\ntest_data = pd.read_excel(\"/kaggle/input/test-data-suffled-and-removed/new Test Data_suffled.xlsx\")\n# Define the allowed labels\nallowed_labels = ['entailment', 'contradiction', 'neutral']\ntest_labels = ['contradiction', 'entailment', 'neutral'] \n\n# Filter the training dataset to keep only rows with allowed labels\ntrain_data = train_data[train_data['label'].isin(allowed_labels)]\n\n# Filter the validation dataset to keep only rows with allowed labels\nval_data = val_data[val_data['label'].isin(allowed_labels)]\n\n# Filter the test dataset to keep only rows with allowed labels\ntest_data = test_data[test_data['label'].isin(allowed_labels)]","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:56.199121Z","iopub.execute_input":"2024-02-27T20:16:56.199338Z","iopub.status.idle":"2024-02-27T20:16:57.936871Z","shell.execute_reply.started":"2024-02-27T20:16:56.199314Z","shell.execute_reply":"2024-02-27T20:16:57.935947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract unique labels from each dataset\nunique_train_labels = train_data['label'].unique()\nunique_val_labels = val_data['label'].unique()\nunique_test_labels = test_data['label'].unique()\n\n# Print the unique labels for each dataset\nprint(\"Unique Labels in Training Data:\", unique_train_labels)\nprint(\"Unique Labels in Validation Data:\", unique_val_labels)\nprint(\"Unique Labels in Test Data:\", unique_test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:57.938065Z","iopub.execute_input":"2024-02-27T20:16:57.938367Z","iopub.status.idle":"2024-02-27T20:16:57.945586Z","shell.execute_reply.started":"2024-02-27T20:16:57.938332Z","shell.execute_reply":"2024-02-27T20:16:57.944867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the labels\nlabel_encoder = LabelEncoder()\ntrain_data['label'] = label_encoder.fit_transform(train_data['label'])\nval_data['label'] = label_encoder.transform(val_data['label'])\ntest_data['label'] = label_encoder.transform(test_data['label'])\nunique_labels = train_data['label'].unique()\n\n# Tokenize and prepare the data\nclass TextEntailmentDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        premise = self.data.iloc[idx]['sentence']\n        hypothesis = self.data.iloc[idx]['output']\n        label = self.data.iloc[idx]['label']\n\n        encoding = self.tokenizer(premise, hypothesis, padding='max_length', truncation=True, max_length=self.max_length)\n        inputs = {k: torch.tensor(v) for k, v in encoding.items()}\n        inputs['labels'] = torch.tensor(label)\n\n        return inputs\n\n# LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.lstm1 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n        self.lstm2 = nn.LSTM(hidden_size, 64, batch_first=True)\n        self.dropout = nn.Dropout(0.2)\n        self.dense1 = nn.Linear(64, 1)  # Linear layer without activation\n        self.relu = nn.ReLU()  # ReLU activation function\n        self.dense2 = nn.Linear(1, output_size) \n\n    def forward(self, x):\n        x = self.embedding(x)\n        lstm_out1, _ = self.lstm1(x)\n        lstm_out2, _ = self.lstm2(lstm_out1[:, -1, :])\n        x = self.dropout(lstm_out2)\n        x = self.dense1(x)\n        x = self.relu(x)\n        output = self.dense2(x)\n        return output\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n# Initialize the LSTM model\ninput_size = len(tokenizer.get_vocab())\nhidden_size = 128\noutput_size = len(unique_labels)\nlstm_model = LSTMModel(input_size, hidden_size, output_size)\n\n# Create data loaders\ntrain_dataset = TextEntailmentDataset(train_data, tokenizer)\nval_dataset = TextEntailmentDataset(val_data, tokenizer)\ntest_dataset = TextEntailmentDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\ntest_loader = DataLoader(test_dataset, batch_size=64)\n\n# Define training parameters\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lstm_model.parameters(), lr=1e-3)\nnum_epochs = 30\n\n# Training loop\nfor epoch in range(num_epochs):\n    lstm_model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        inputs, labels = batch['input_ids'], batch['labels']\n        outputs = lstm_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    # Validation loop\n    lstm_model.eval()\n    true_labels = []\n    predicted_labels = []\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, labels = batch['input_ids'], batch['labels']\n            outputs = lstm_model(inputs)\n            predictions = torch.argmax(outputs, dim=1)\n            true_labels.extend(labels.cpu().numpy())\n            predicted_labels.extend(predictions.cpu().numpy())\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n\n        accuracy = correct / total\n        print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n# Test loop\nlstm_model.eval()\ntrue_labels = []\npredicted_labels = []\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs, labels = batch['input_ids'], batch['labels']\n        outputs = lstm_model(inputs)\n        predictions = torch.argmax(outputs, dim=1)\n        true_labels.extend(labels.cpu().numpy())\n        predicted_labels.extend(predictions.cpu().numpy())\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\n    accuracy = correct / total\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Print classification report\nprint(classification_report(true_labels, predicted_labels, target_names=test_labels))","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:29:44.035018Z","iopub.execute_input":"2024-02-27T20:29:44.035359Z","iopub.status.idle":"2024-02-27T20:42:45.163881Z","shell.execute_reply.started":"2024-02-27T20:29:44.035332Z","shell.execute_reply":"2024-02-27T20:42:45.163011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:58.462607Z","iopub.status.idle":"2024-02-27T20:16:58.462925Z","shell.execute_reply.started":"2024-02-27T20:16:58.462772Z","shell.execute_reply":"2024-02-27T20:16:58.462793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:58.463914Z","iopub.status.idle":"2024-02-27T20:16:58.464196Z","shell.execute_reply.started":"2024-02-27T20:16:58.464058Z","shell.execute_reply":"2024-02-27T20:16:58.464072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:58.465022Z","iopub.status.idle":"2024-02-27T20:16:58.465295Z","shell.execute_reply.started":"2024-02-27T20:16:58.465159Z","shell.execute_reply":"2024-02-27T20:16:58.465174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:58.466430Z","iopub.status.idle":"2024-02-27T20:16:58.466793Z","shell.execute_reply.started":"2024-02-27T20:16:58.466619Z","shell.execute_reply":"2024-02-27T20:16:58.466637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T20:16:58.467976Z","iopub.status.idle":"2024-02-27T20:16:58.468301Z","shell.execute_reply.started":"2024-02-27T20:16:58.468139Z","shell.execute_reply":"2024-02-27T20:16:58.468156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}